{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab26840",
   "metadata": {},
   "source": [
    "## 파이토치 기초(4) - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3c530",
   "metadata": {},
   "source": [
    "* day 1 - 텐서와 Autograd : https://dacon.io/codeshare/4478\n",
    "* day 2 - 신경망모델 구현하기 : https://dacon.io/codeshare/4495\n",
    "* day 3 - DNN : https://dacon.io/codeshare/4532\n",
    "* 오늘은 영상처리에 탁월한 성능을 자랑하는 CNN의 원리를 알아보고, Fashion MNIST에 적용해본 후, CNN을 이용한 ResNet모델로 좀 더 복잡한 컬러 이미지까지 다뤄보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5416ed",
   "metadata": {},
   "source": [
    "* 컴퓨터에서 보는 모든 이미지는 픽셀값들을 가로, 세로로 늘어놓은 행렬로 표현할 수 있습니다.\n",
    "* 컨볼루션은 계층적으로 이미지를 인식하 수 있도록 단계마다 이미지의 특징을 추출하는 것을 말합니다.\n",
    "* CNN은 이미지를 추출하는 필터로 Convolution Neural Network 즉 컨볼루션을 하는 인공 신경망입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fb873",
   "metadata": {},
   "source": [
    "* CNN 모델은 일반적으로 컨볼루션 계층(convolution layer), 풀링 계층(pooling layer), 특징들을 모아 최종 분류하는 일반적인 인공신경망 계층으로 구성됩니다.\n",
    "* 컨볼루션을 거쳐 만들어진 새로운 이미지는 특징 맵(feature map)이라고도 불립니다. 컨볼루션 계층마다 여러 특징 맵들이 만들어지며, 다음 단계인 풀링(pooling) 계층으로 넘어가게 됩니다. 컨볼루션 계층과 풀링 계층을 여러 겹 쌓아, 각 단계에서 만들어진 특징 맵을 관찰하면 CNN 모델이 이미지를 계층적으로 인식하는 것을 볼 수 있습니다.\n",
    "* 특징 맵의 크기가 크면 학습이 어렵고 과적합의 위험이 증가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351c145",
   "metadata": {},
   "source": [
    "### CNN 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517924c",
   "metadata": {},
   "source": [
    "* 여러 CNN 모델은 컨볼루션, 풀링, 드롭아웃, 그리고 일반적인 신경망 계층의 조합으로 이루어집니다.\n",
    "* 컨볼루션 -> 풀링 -> 컨볼루션 -> 드롭아웃 -> 풀링 -> 신경망 -> 드롭아웃 -> 신경망의 예제를 구현해보겠습니다.\n",
    "* 일반 인공신경망을 CNN 계층으로 대체하면 되기 때문에 전체적 구현은 day-3의 신경망 구현법과 매우 비슷합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847297ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T08:01:13.322689Z",
     "start_time": "2022-02-15T08:01:04.960045Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a31a05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T08:06:35.677125Z",
     "start_time": "2022-02-15T08:06:35.668148Z"
    }
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "#USE_CUDA는 CUDA를 사용할 수 있는지 확인하는 코드이고, DEVICE는 USE_CUDA의 결과에 따라 cpu를 쓸지 gpu를 쓸지 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0037a1eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T08:07:04.189200Z",
     "start_time": "2022-02-15T08:07:04.181220Z"
    }
   },
   "outputs": [],
   "source": [
    "# 이폭과 배치크기를 정해줍니다\n",
    "EPOCHS     = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a893f589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T08:08:01.943026Z",
     "start_time": "2022-02-15T08:07:53.164495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2a20429ac84b4f93fed6081f03627e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea430cfa03140acaf01402a355b6409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf72b4504004380b82391f467ccb958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da1590a5a3848ba804ba77c052d6b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fashion MNIST 데이터셋을 불러옵니다.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./.data',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([ #transforms를 이용한 전처리는 파이토치 텐서화와 정규화만 하였습니다.\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./.data',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c86d8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T08:22:25.135506Z",
     "start_time": "2022-02-15T08:22:25.115558Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습하기\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) #만드는 모델의 커널크기는 5x5입니다. 숫자를 하나만 지정하면 정사각형으로 간주합니다\n",
    "        # nn.Conv2d는 입력 x를 받는 함수를 반환합니다. \n",
    "        # nn.Conv2d의 첫 두 파라미터는 입력 채널수(in_channels)와 출력 채널수(out_channels)입니다.\n",
    "        # 첫 컨볼루션 계층에서는(self.conv1) 10개의 특징맵을 생성합니다\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # 두번째 컨볼루션 게층에서는 10개의 특징맵을 받아 20개의 특징맵을 만듭니다.\n",
    "        self.conv2_drop = nn.Dropout2d() \n",
    "        # 컨볼루션 결과 출력값에는 드롭아웃을 해줍니다. nn.Dropout2d 모듈로 드롭아웃 인스턴스를 만들 수 있습니다.\n",
    "        self.fc1 = nn.Linear(320, 50) # 컨볼루션과 드롭아웃을 거친 이미지는 nn.Linear의 일반 신경망을 거칩니다.\n",
    "        self.fc2 = nn.Linear(50, 10) # 입력크기 50, 출력은 분류할 클래스 개수인 10으로 설정합니다. (각 계층의 출력크기는 임의로 지정)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # 입력 받은 것이 첫 컨볼루션 계층을 거치고 F.max_pool2d함수를 거치게 합니다.\n",
    "        # F.max_pool2d의 두 번째 입력은 커널 크기입니다.(2)\n",
    "        # 컨볼루션과 맥스 풀링을 통과한 x는 F.relu()활성화 함수를 거칩니다\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) # 두번째 컨볼루션 계층도 똑같이 반복합니다\n",
    "        x = x.view(-1, 320) # 컨볼루션 계층 2개를 거쳐 특징맵이 된 x를 1차원으로 펴줍니다. (-1은 남는차원 모두, 320은 x가 가진 원소개수)\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.dropout(x, training=self.training) # ReLU 활성화 함수를 거친 뒤 드롭아웃을 사용합니다\n",
    "        x = self.fc2(x) # 0부터 9까지 레이블을 갖는 10개의 출력값을 가지는 신경망\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1543694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T08:23:32.106536Z",
     "start_time": "2022-02-15T08:23:32.076613Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파라미터를 지정합니다.\n",
    "model     = Net().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) #최적화 알고리즘으로 파이토치에 내장되어 있는 optim.SGD를 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03325db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T08:25:15.496653Z",
     "start_time": "2022-02-15T08:25:15.474707Z"
    }
   },
   "outputs": [],
   "source": [
    "# 앞 장에서 본 과정과 완전히 동일한 모델 훈련과 평가코드입니다.\n",
    "# 훈련코드\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# 성능확인코드\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fabe3815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T09:02:45.707138Z",
     "start_time": "2022-02-15T08:25:32.800389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303096\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.018277\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.267298\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.469899\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.620442\n",
      "[1] Test Loss: 0.2143, Accuracy: 93.83%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.500467\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.467868\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.372048\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.184066\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.389603\n",
      "[2] Test Loss: 0.1278, Accuracy: 96.18%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.127458\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.555271\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.164062\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.325616\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.478401\n",
      "[3] Test Loss: 0.0970, Accuracy: 97.18%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.252095\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.177119\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.286993\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.262875\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.226527\n",
      "[4] Test Loss: 0.0845, Accuracy: 97.45%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.239577\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.199878\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.224258\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.255741\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.293300\n",
      "[5] Test Loss: 0.0706, Accuracy: 97.68%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.242005\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.245277\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.263292\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.153598\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.154065\n",
      "[6] Test Loss: 0.0636, Accuracy: 98.01%\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.201028\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.465392\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.198257\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.386086\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.179007\n",
      "[7] Test Loss: 0.0595, Accuracy: 98.06%\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.115172\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.238662\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.246885\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.332805\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.149733\n",
      "[8] Test Loss: 0.0547, Accuracy: 98.33%\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.375447\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.135098\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.181123\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.357397\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.153945\n",
      "[9] Test Loss: 0.0529, Accuracy: 98.41%\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.126349\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.174319\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.150500\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.060842\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.273812\n",
      "[10] Test Loss: 0.0514, Accuracy: 98.42%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.088693\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.137272\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.389275\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.247162\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.162257\n",
      "[11] Test Loss: 0.0493, Accuracy: 98.46%\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.075318\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.186522\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.290097\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.081257\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.157084\n",
      "[12] Test Loss: 0.0441, Accuracy: 98.61%\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.179603\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.158531\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.260254\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.141902\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.087700\n",
      "[13] Test Loss: 0.0436, Accuracy: 98.63%\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.184320\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.335802\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.176613\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.130347\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.067368\n",
      "[14] Test Loss: 0.0436, Accuracy: 98.71%\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.170986\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.120949\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.068376\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.126922\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.259227\n",
      "[15] Test Loss: 0.0451, Accuracy: 98.60%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.261036\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.064939\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.150518\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.097378\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.043312\n",
      "[16] Test Loss: 0.0407, Accuracy: 98.72%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.154460\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.112751\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.241530\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.068960\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.091542\n",
      "[17] Test Loss: 0.0403, Accuracy: 98.74%\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.051251\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.089053\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.225500\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.106595\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.042056\n",
      "[18] Test Loss: 0.0393, Accuracy: 98.75%\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.034896\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.225806\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.232235\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.062142\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.110965\n",
      "[19] Test Loss: 0.0377, Accuracy: 98.84%\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.207861\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.114367\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.119548\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.106726\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.128416\n",
      "[20] Test Loss: 0.0359, Accuracy: 98.90%\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.062855\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.071011\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.212846\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.081860\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.058967\n",
      "[21] Test Loss: 0.0347, Accuracy: 98.85%\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.135004\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.198076\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.212130\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.339400\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.055321\n",
      "[22] Test Loss: 0.0368, Accuracy: 98.93%\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.021615\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.192455\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.074765\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.151564\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.048445\n",
      "[23] Test Loss: 0.0379, Accuracy: 98.78%\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.145562\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.119751\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.159876\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.060291\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.133607\n",
      "[24] Test Loss: 0.0346, Accuracy: 98.90%\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.036625\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.124076\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.030132\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.160908\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.228688\n",
      "[25] Test Loss: 0.0356, Accuracy: 98.89%\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.072176\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.073932\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.114173\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.058027\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.052524\n",
      "[26] Test Loss: 0.0350, Accuracy: 98.95%\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.373343\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.119521\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.117647\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.149056\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.258303\n",
      "[27] Test Loss: 0.0338, Accuracy: 99.02%\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.119153\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.065341\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.027587\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.108567\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.087605\n",
      "[28] Test Loss: 0.0336, Accuracy: 98.92%\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.094227\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.093680\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.081095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.117330\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.162402\n",
      "[29] Test Loss: 0.0358, Accuracy: 98.90%\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.369476\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.072453\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.279839\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.031995\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.071114\n",
      "[30] Test Loss: 0.0342, Accuracy: 98.99%\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.075928\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.102470\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.084451\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.055920\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.218221\n",
      "[31] Test Loss: 0.0339, Accuracy: 98.93%\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.287325\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.148126\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.083411\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.216294\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.149540\n",
      "[32] Test Loss: 0.0322, Accuracy: 99.00%\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.063353\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.082261\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.184042\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.107573\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.048825\n",
      "[33] Test Loss: 0.0328, Accuracy: 99.00%\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.258947\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.036005\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.216005\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.127290\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.177995\n",
      "[34] Test Loss: 0.0331, Accuracy: 99.01%\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.084228\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.032822\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.069928\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.028321\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.158685\n",
      "[35] Test Loss: 0.0321, Accuracy: 99.12%\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.062457\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.232712\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.085360\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.010284\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.186611\n",
      "[36] Test Loss: 0.0329, Accuracy: 99.04%\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.100679\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.042619\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.080347\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.058407\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.163926\n",
      "[37] Test Loss: 0.0318, Accuracy: 99.02%\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.163788\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.108624\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.070824\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.417311\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.066140\n",
      "[38] Test Loss: 0.0311, Accuracy: 99.06%\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.065350\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.032671\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.042980\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.104812\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.060994\n",
      "[39] Test Loss: 0.0315, Accuracy: 99.05%\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.029690\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.104349\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.062758\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.037230\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.069977\n",
      "[40] Test Loss: 0.0324, Accuracy: 99.01%\n"
     ]
    }
   ],
   "source": [
    "# 코드 실행\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy)) # day3에서와 다르게 정확도가 99%까지 올라간 것을 볼 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814b98e",
   "metadata": {},
   "source": [
    "### ResNet으로 컬러 데이터셋에 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f46d1",
   "metadata": {},
   "source": [
    "* ResNet은 이미지넷 대회에서 2015년에 우승한 모델로, 신경망을 깊게 쌓으면 오히려 성능이 나빠지는 문제를 해결하는 방법을 제시했고, 이후 DenseNet 등의 파생모델에 영향을 주었습니다.\n",
    "* 이번에는 Fashion MNIST 대신 CIFAR-10 데이터셋을 사용하겠습니다. CIFAR-10 데이터셋은 32x32 크기의 컬러 이미지 6만개를 포함하고 있으며 자동차, 새, 고양이, 사슴 등 10가지 분류가 존재합니다.\n",
    "* 컬러 이미지의 픽셀값은 몇 가지 채널(channel)로 구성되는데, 채널이란 이미지 구성요소를 가리킵니다. 오늘날 가장 널리 쓰이는 24bit 컬러 이미지는 R,G,B 각각에 8bit(0~255)씩 색상값을 가지는 3가지 채널을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d704c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T09:11:18.742028Z",
     "start_time": "2022-02-15T09:09:49.627214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./.data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dfc755e3b44f6d811715d232b90272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\cifar-10-python.tar.gz to ./.data\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data', #CIFRA-10 데이터를 불러옵니다.\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.RandomCrop(32, padding=4),\n",
    "                       transforms.RandomHorizontalFlip(), # 과적합 방지를 위해 RandomCrop과 RandomHorizontalFlip을 추가했습니다\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa66793",
   "metadata": {},
   "source": [
    "* ResNet은 여러 단계의 신경망을 거치며 최초 입력 이미지에 대한 정보가 소실되는 문제를 해결하는 방안을 제시합니다.\n",
    "* ResNet은 네트워크를 작은 블록인 Residual 블록으로 나누어 Residual 블록의 출력에 입력이었던 x를 더함으로써 모델을 훨씬 깊게 설계할 수 있도록 했습니다.\n",
    "* 입력과 출력의 관계를 바로 학습하기보다 입력과 출력의 차이를 따로 학습하는 것이 성능이 좋다는 가설입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa49b4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T09:28:20.425528Z",
     "start_time": "2022-02-15T09:28:20.397606Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델만들기\n",
    "\n",
    "class BasicBlock(nn.Module): # Residual 블록을 BasicBlock이라는 새로운 파이토치 모듈로 정의해서 사용합니다.\n",
    "    # 파이토치는 nn.Module을 이용하여 모듈위에 또다른 모듈을 쌓아 올릴 수 있습니다.\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        # nn.BatchNorm2d는 배치 정규화(batch normalization)을 수행하는 계층입니다.\n",
    "        # 학습률을 너무 높게 잡았을 때 기울기가 소실되거나 발산하는 증상을 예방하여 학습 과정을 안정화합니다.\n",
    "        # 즉, 이 계층은 자체적으로 정규화를 수행해 드롭아웃과 같은 효과를 내는 장점이 있습니다.\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    # ResNet의 두번째 블록 부터는 in_planes()를 받아 self.bn2 계층과 출력크기가 같은 planes를 더해주는 self.shortcut모듈을 정의합니다\n",
    "        self.shortcut = nn.Sequential() # nn.Sequential()은 여러 모듈(nn.Module)을 하나의 모듈로 엮는 역할을 합니다.\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x): #데이터의 흐름은 앞서 한 설명과 같습니다.\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# 모델정의\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16 # self.in_planes 변수는 _make_layer 함수가 층을 만들 때 채널 출력값을 기록하는 데 쓰입니다.\n",
    "        # layer1이 입력받는 채널의 개수가 16개이므로 16으로 초기화해줍니다.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1) \n",
    "        #self._make_layer()는 nn.Sequential의 도구로 여러 BasicBlock 모듈을 하나로 묶어주는 역할을 합니다.\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        # self.layer1~3은 컨볼루션 계층과 마찬가지로 모듈(nn.Module)로 취급하면 됩니다.\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    # _make_layer 함수는 self.in_planes 채널 개수로부터 직접 입력받은 인수인 planes 채널 개수만큼을 출력하는 BasicBlock을 생성합니다.\n",
    "    # layer1 : 16채널에서 16채널을 보내는 BasicBlock 2개\n",
    "    # layer2 : 16채널을 받아 32채널을 출력하는 BasicBlock 1개와 32채널에서 32채널을 내보내는 BasicBlock 1개\n",
    "    # layer3 : 32채널을 받아 64채널을 출력하는 BasicBlock 1개와 64채널에서 64채널을 출력하는 BasicBlock 1개\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    # ResNet 모델은 위와같이 컨볼루션, 배치정규화, 활성화 함수를 통과하고 사전에 정의해둔 BasicBlock 층을 가지고 있는\n",
    "    # layer1, layer2, layer3를 통과하게 됩니다. 각 layer는 2개의 Residual 블록을 갖고 있고 \n",
    "    # 이렇게 나온 값에 평균 풀링을 하고 마지막 계층을 거쳐 분류결과를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47b85b",
   "metadata": {},
   "source": [
    "* 이번 예제에서는 학습률 감소(learning rate decay)기법을 사용합니다.\n",
    "* 학습률 감소는 학습이 진행하면서 최적화 함수의 학습률을 점점 낮춰서 더 정교하게 최적화합니다. \n",
    "* 이는 파이토치 내부의 optim.lr_scheduler.StepLR 도구로 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6239dc86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T09:31:29.068800Z",
     "start_time": "2022-02-15T09:31:29.005007Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005) \n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "# scheduler는 이폭마다 호출되며 step_size를 50으로 지정해 50번 호출될 때 학습률에 0.1(gamma)만큼 곱합니다.\n",
    "# 즉 0.1로 시작한 학습률은 50이폭 이후에 0.1 x 0.1 = 0.01로 낮아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e5e191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T09:32:12.732053Z",
     "start_time": "2022-02-15T09:32:12.717090Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(model) #처음부터 끝까지 모든 계층이 어떻게 생겼는지 print(model)로 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb488f7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T14:38:55.663469Z",
     "start_time": "2022-02-15T09:33:07.652382Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syi06\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.559069\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.842535\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.675769\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.403505\n",
      "[1] Test Loss: 1.5372, Accuracy: 48.17%\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.265703\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.304142\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.890852\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.127497\n",
      "[2] Test Loss: 1.0901, Accuracy: 62.10%\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.010721\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.851191\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.896181\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.755468\n",
      "[3] Test Loss: 1.0075, Accuracy: 65.59%\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.967926\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.845373\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.778225\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.230998\n",
      "[4] Test Loss: 1.1727, Accuracy: 61.22%\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.105525\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.028799\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.617465\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.744107\n",
      "[5] Test Loss: 0.8786, Accuracy: 70.02%\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.912479\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.529713\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.659153\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.827887\n",
      "[6] Test Loss: 0.9309, Accuracy: 69.25%\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.716527\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.968956\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.837302\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.728246\n",
      "[7] Test Loss: 1.1088, Accuracy: 63.49%\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.691901\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.511556\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.590451\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.568654\n",
      "[8] Test Loss: 0.8888, Accuracy: 70.46%\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.758420\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.741377\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.731530\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.664674\n",
      "[9] Test Loss: 1.0239, Accuracy: 67.41%\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.790774\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.809218\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.579479\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.926438\n",
      "[10] Test Loss: 0.7879, Accuracy: 72.78%\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.858985\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.576833\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.862380\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.806077\n",
      "[11] Test Loss: 0.8438, Accuracy: 71.82%\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.469263\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.606098\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.500929\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.696833\n",
      "[12] Test Loss: 0.8706, Accuracy: 71.34%\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.534497\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.637210\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.681901\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.600698\n",
      "[13] Test Loss: 0.7614, Accuracy: 75.52%\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.540390\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.563288\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.697792\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.607243\n",
      "[14] Test Loss: 0.9026, Accuracy: 70.33%\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.819507\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.439723\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.836854\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.512615\n",
      "[15] Test Loss: 0.8866, Accuracy: 71.17%\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.842134\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 0.481420\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.600881\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.526269\n",
      "[16] Test Loss: 0.7616, Accuracy: 74.00%\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.614237\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.766598\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.498433\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.697574\n",
      "[17] Test Loss: 0.8209, Accuracy: 72.47%\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.561456\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.637866\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.583483\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.547721\n",
      "[18] Test Loss: 1.0389, Accuracy: 67.54%\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.798559\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.772825\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.343184\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.640341\n",
      "[19] Test Loss: 1.4315, Accuracy: 62.21%\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.364476\n",
      "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 0.862341\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.676526\n",
      "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.648322\n",
      "[20] Test Loss: 0.7731, Accuracy: 74.44%\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.592895\n",
      "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.773509\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.523916\n",
      "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.675831\n",
      "[21] Test Loss: 0.9264, Accuracy: 71.49%\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.529945\n",
      "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.819448\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.597296\n",
      "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.724750\n",
      "[22] Test Loss: 0.8158, Accuracy: 72.82%\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.727738\n",
      "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.440964\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.646798\n",
      "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.708858\n",
      "[23] Test Loss: 0.7869, Accuracy: 74.82%\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.765507\n",
      "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 0.557613\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.497647\n",
      "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.771618\n",
      "[24] Test Loss: 0.6609, Accuracy: 77.02%\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.787489\n",
      "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.562417\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.571493\n",
      "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.625465\n",
      "[25] Test Loss: 0.7136, Accuracy: 75.47%\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.578042\n",
      "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 0.565719\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.555556\n",
      "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.578081\n",
      "[26] Test Loss: 0.8588, Accuracy: 72.39%\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.594252\n",
      "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 0.658834\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.532882\n",
      "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.551021\n",
      "[27] Test Loss: 1.3824, Accuracy: 57.10%\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.744946\n",
      "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 0.504354\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.701046\n",
      "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.680752\n",
      "[28] Test Loss: 0.8926, Accuracy: 71.49%\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.613764\n",
      "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 0.388292\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.594227\n",
      "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.535013\n",
      "[29] Test Loss: 0.8152, Accuracy: 72.53%\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.486560\n",
      "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 0.600452\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.780243\n",
      "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 0.726135\n",
      "[30] Test Loss: 0.9125, Accuracy: 71.48%\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.631872\n",
      "Train Epoch: 31 [12800/50000 (26%)]\tLoss: 0.578651\n",
      "Train Epoch: 31 [25600/50000 (51%)]\tLoss: 0.620267\n",
      "Train Epoch: 31 [38400/50000 (77%)]\tLoss: 0.612486\n",
      "[31] Test Loss: 1.0532, Accuracy: 66.34%\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.494051\n",
      "Train Epoch: 32 [12800/50000 (26%)]\tLoss: 0.672099\n",
      "Train Epoch: 32 [25600/50000 (51%)]\tLoss: 0.773423\n",
      "Train Epoch: 32 [38400/50000 (77%)]\tLoss: 0.780342\n",
      "[32] Test Loss: 0.9151, Accuracy: 71.13%\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.655824\n",
      "Train Epoch: 33 [12800/50000 (26%)]\tLoss: 0.555209\n",
      "Train Epoch: 33 [25600/50000 (51%)]\tLoss: 0.476366\n",
      "Train Epoch: 33 [38400/50000 (77%)]\tLoss: 0.676064\n",
      "[33] Test Loss: 0.8776, Accuracy: 72.42%\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.532035\n",
      "Train Epoch: 34 [12800/50000 (26%)]\tLoss: 0.568952\n",
      "Train Epoch: 34 [25600/50000 (51%)]\tLoss: 0.706934\n",
      "Train Epoch: 34 [38400/50000 (77%)]\tLoss: 0.597727\n",
      "[34] Test Loss: 0.8678, Accuracy: 71.44%\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.883245\n",
      "Train Epoch: 35 [12800/50000 (26%)]\tLoss: 0.618421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35 [25600/50000 (51%)]\tLoss: 0.520221\n",
      "Train Epoch: 35 [38400/50000 (77%)]\tLoss: 0.527956\n",
      "[35] Test Loss: 0.9450, Accuracy: 71.38%\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.575580\n",
      "Train Epoch: 36 [12800/50000 (26%)]\tLoss: 0.529518\n",
      "Train Epoch: 36 [25600/50000 (51%)]\tLoss: 0.709076\n",
      "Train Epoch: 36 [38400/50000 (77%)]\tLoss: 0.568758\n",
      "[36] Test Loss: 0.8203, Accuracy: 71.90%\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.406103\n",
      "Train Epoch: 37 [12800/50000 (26%)]\tLoss: 0.594710\n",
      "Train Epoch: 37 [25600/50000 (51%)]\tLoss: 0.374258\n",
      "Train Epoch: 37 [38400/50000 (77%)]\tLoss: 0.765149\n",
      "[37] Test Loss: 0.7800, Accuracy: 74.44%\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.776516\n",
      "Train Epoch: 38 [12800/50000 (26%)]\tLoss: 0.512619\n",
      "Train Epoch: 38 [25600/50000 (51%)]\tLoss: 0.468177\n",
      "Train Epoch: 38 [38400/50000 (77%)]\tLoss: 0.668412\n",
      "[38] Test Loss: 1.3345, Accuracy: 65.05%\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.736510\n",
      "Train Epoch: 39 [12800/50000 (26%)]\tLoss: 0.506434\n",
      "Train Epoch: 39 [25600/50000 (51%)]\tLoss: 0.698928\n",
      "Train Epoch: 39 [38400/50000 (77%)]\tLoss: 0.563656\n",
      "[39] Test Loss: 0.6891, Accuracy: 76.73%\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.668114\n",
      "Train Epoch: 40 [12800/50000 (26%)]\tLoss: 0.568694\n",
      "Train Epoch: 40 [25600/50000 (51%)]\tLoss: 0.664247\n",
      "Train Epoch: 40 [38400/50000 (77%)]\tLoss: 0.744241\n",
      "[40] Test Loss: 0.9173, Accuracy: 71.45%\n"
     ]
    }
   ],
   "source": [
    "# 훈련하기 (코드 돌리기)\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    scheduler.step() #코드는 위와 대부분 동일하나 scheduler.step()으로 학습률을 조금 낮춰주는 단계가 추가되었습니다.\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
